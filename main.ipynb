{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV33AKLtMu8i"
      },
      "source": [
        "# Analyse Spectrale des Réseaux de Neurones avec la Loi de Marchenko-Pastur\n",
        "\n",
        "## Objectif\n",
        "\n",
        "Ce notebook permet de comprendre et analyser les poids d'un réseau de neurones en utilisant la théorie des matrices aléatoires. Plus précisément, on va utiliser la **loi de Marchenko-Pastur** pour détecter les signaux importants dans les poids du réseau.\n",
        "\n",
        "## Contexte\n",
        "\n",
        "Quand on entraîne un réseau de neurones, les poids qu'il apprend contiennent à la fois :\n",
        "- Du **signal** : des informations utiles que le réseau a appris\n",
        "- Du **bruit** : des valeurs aléatoires qui ne servent pas vraiment\n",
        "\n",
        "La loi de Marchenko-Pastur nous aide à faire la différence entre ces deux types de valeurs. On peut ensuite **élaguer** (pruning) le réseau en supprimant le bruit, ce qui le rend plus petit et plus rapide sans perdre en performance.\n",
        "\n",
        "## Plan du Notebook\n",
        "\n",
        "1. Définir la loi de Marchenko-Pastur\n",
        "2. Vérifier qu'elle fonctionne sur une matrice aléatoire pure\n",
        "3. Tester sur une matrice avec du signal ajouté\n",
        "4. Entraîner un réseau de neurones sur MNIST\n",
        "5. Analyser les poids du réseau\n",
        "6. Élaguer le réseau en gardant seulement le signal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZLC1IekMu8k"
      },
      "source": [
        "## 1. Imports et Configuration\n",
        "\n",
        "On commence par importer toutes les bibliothèques nécessaires :\n",
        "- **numpy** : pour les calculs numériques\n",
        "- **matplotlib** : pour les graphiques\n",
        "- **scipy** : pour l'algèbre linéaire (calcul des valeurs propres)\n",
        "- **torch** : pour créer et entraîner le réseau de neurones\n",
        "- **torchvision** : pour charger le dataset MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCuo4g3_Mu8k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import linalg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# On fixe les graines aléatoires pour avoir des résultats reproductibles\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQZvCk5vMu8q"
      },
      "source": [
        "## 2. Théorie : La Loi de Marchenko-Pastur\n",
        "\n",
        "### Qu'est-ce que c'est ?\n",
        "\n",
        "Imaginons qu'on a une grande matrice **W** de taille N×M remplie de nombres aléatoires. La loi de Marchenko-Pastur nous dit comment se distribuent les **valeurs propres** de cette matrice.\n",
        "\n",
        "### Formule mathématique\n",
        "\n",
        "Pour une matrice aléatoire N×M avec :\n",
        "- **c = M/N** (rapport entre nombre de colonnes et lignes)\n",
        "- **σ²** (variance des éléments)\n",
        "\n",
        "Les valeurs propres sont concentrées entre deux bornes :\n",
        "- **λ_min** = σ² × (1 - √c)²\n",
        "- **λ_max** = σ² × (1 + √c)²\n",
        "\n",
        "Toute valeur propre **au-dessus de λ_max** représente du **signal réel**, pas du bruit aléatoire !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raTVnOvfMu8r"
      },
      "outputs": [],
      "source": [
        "def marchenko_pastur(x, sigma2, c):\n",
        "    \"\"\"\n",
        "    Calcule la densité de la loi de Marchenko-Pastur.\n",
        "\n",
        "    Paramètres :\n",
        "    -----------\n",
        "    x : array ou None\n",
        "        Points où on veut calculer la densité. Si None, retourne seulement les bornes.\n",
        "    sigma2 : float\n",
        "        Variance des éléments de la matrice\n",
        "    c : float\n",
        "        Rapport M/N (colonnes/lignes)\n",
        "\n",
        "    Retour :\n",
        "    --------\n",
        "    rho : densité de probabilité aux points x\n",
        "    lam_min : borne inférieure de la distribution\n",
        "    lam_max : borne supérieure de la distribution (seuil important !)\n",
        "    \"\"\"\n",
        "    # Calcul des bornes théoriques\n",
        "    lam_min = sigma2 * (1 - np.sqrt(c))**2\n",
        "    lam_max = sigma2 * (1 + np.sqrt(c))**2\n",
        "\n",
        "    # Si on veut juste les bornes, on s'arrête là\n",
        "    if x is None:\n",
        "        return None, lam_min, lam_max\n",
        "\n",
        "    # Calcul de la densité pour chaque point x\n",
        "    rho = np.zeros_like(x)\n",
        "    mask = (x >= lam_min) & (x <= lam_max)  # On ne calcule que dans l'intervalle valide\n",
        "\n",
        "    # Pour éviter les divisions par zéro\n",
        "    safe_x = x.copy()\n",
        "    safe_x[safe_x == 0] = 1e-16\n",
        "\n",
        "    # Formule de la densité MP\n",
        "    rho[mask] = np.sqrt((lam_max - x[mask]) * (x[mask] - lam_min)) / \\\n",
        "                (2 * np.pi * sigma2 * c * safe_x[mask])\n",
        "\n",
        "    return rho, lam_min, lam_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UbKYx5fMu8w"
      },
      "outputs": [],
      "source": [
        "def compute_esd(W):\n",
        "    \"\"\"\n",
        "    Calcule la distribution spectrale empirique (ESD) d'une matrice W.\n",
        "\n",
        "    On calcule X = (1/N) × W^T × W, puis on récupère ses valeurs propres.\n",
        "    Ces valeurs propres nous disent comment l'énergie est répartie dans la matrice.\n",
        "\n",
        "    Paramètres :\n",
        "    -----------\n",
        "    W : array de taille N×M\n",
        "        La matrice à analyser\n",
        "\n",
        "    Retour :\n",
        "    --------\n",
        "    eigenvalues : array trié\n",
        "        Les valeurs propres de X, triées par ordre croissant\n",
        "    \"\"\"\n",
        "    N, M = W.shape\n",
        "    X = W.T @ W / N  # Matrice de covariance normalisée\n",
        "\n",
        "    # eigvalsh est optimisé pour les matrices symétriques (comme X)\n",
        "    return np.sort(linalg.eigvalsh(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yQsbrSDMu8w"
      },
      "source": [
        "## 3. Test 1 : Vérification sur une Matrice Aléatoire Pure\n",
        "\n",
        "### Pourquoi ce test ?\n",
        "\n",
        "Avant d'utiliser la loi MP sur un vrai réseau de neurones, on vérifie qu'elle fonctionne bien sur une matrice purement aléatoire. Si ça marche ici, on peut avoir confiance pour la suite.\n",
        "\n",
        "### Ce qu'on fait :\n",
        "\n",
        "1. On crée une matrice R de taille 1000×800 avec des valeurs aléatoires normales\n",
        "2. On calcule ses valeurs propres (la distribution empirique)\n",
        "3. On compare avec ce que prédit la théorie MP\n",
        "4. On trace un graphique pour voir si ça correspond"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbJH95CMMu8x"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"TEST 1 : Vérification de la loi MP sur matrice aléatoire\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Création d'une matrice aléatoire\n",
        "N, M = 1000, 800\n",
        "R = np.random.randn(N, M) / np.sqrt(N)  # Normalisation importante !\n",
        "\n",
        "# Calcul des valeurs propres empiriques\n",
        "eigs = compute_esd(R)\n",
        "\n",
        "# Calcul des bornes théoriques MP\n",
        "_, lam_min, lam_plus = marchenko_pastur(None, 1.0 / N, M / N)\n",
        "\n",
        "print(f\"Borne inférieure théorique λ_min : {lam_min:.5e}\")\n",
        "print(f\"Borne supérieure théorique λ+ : {lam_plus:.5e}\")\n",
        "print(f\"Valeur propre minimale mesurée : {eigs.min():.5e}\")\n",
        "print(f\"Valeur propre maximale mesurée : {eigs.max():.5e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu7gDCq9Mu8x"
      },
      "outputs": [],
      "source": [
        "# Visualisation\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "# Histogramme des valeurs propres mesurées\n",
        "plt.hist(eigs, bins=50, density=True, alpha=0.6, label='ESD empirique')\n",
        "\n",
        "# Courbe théorique MP\n",
        "x = np.linspace(max(0, eigs.min()*0.9), eigs.max()*1.1, 300)\n",
        "rho, _, _ = marchenko_pastur(x, 1.0 / N, M / N)\n",
        "plt.plot(x, rho, 'r-', lw=2, label='MP théorique')\n",
        "\n",
        "# Ligne verticale pour le seuil λ+\n",
        "plt.axvline(lam_plus, color='g', ls='--', lw=2, label=f'λ+ = {lam_plus:.5e}')\n",
        "\n",
        "plt.xlabel('λ (valeur propre)')\n",
        "plt.ylabel('Densité')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.title('Matrice aléatoire R : théorie vs pratique')\n",
        "\n",
        "print(\"\\n✓ Si les barres bleues suivent bien la courbe rouge, la loi MP fonctionne !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weXbJwl7Mu83"
      },
      "source": [
        "## 4. Test 2 : Modèle avec Signal (Spiked Model)\n",
        "\n",
        "### L'idée\n",
        "\n",
        "Maintenant on va créer une matrice qui contient **du vrai signal** en plus du bruit. On fait ça en ajoutant des \"spikes\" : quelques grandes valeurs singulières.\n",
        "\n",
        "Notre matrice finale est : **W = S + R**\n",
        "- **S** : matrice de signal (rang faible, quelques valeurs singulières grandes)\n",
        "- **R** : matrice de bruit (aléatoire)\n",
        "\n",
        "### Ce qu'on s'attend à voir\n",
        "\n",
        "Les valeurs propres de W devraient :\n",
        "- Suivre la loi MP dans la zone \"bulk\" (bruit)\n",
        "- Dépasser le seuil λ+ pour les spikes (signal)\n",
        "\n",
        "C'est exactement comme ça qu'on pourra détecter le signal dans un réseau de neurones !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MCVuw68Mu83"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTEST 2 : Modèle spiked W = S + R\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Création de la matrice de signal S avec 5 spikes\n",
        "rank_spikes = 5\n",
        "\n",
        "# On crée des bases orthonormales U et V\n",
        "U, _ = np.linalg.qr(np.random.randn(N, rank_spikes))\n",
        "V, _ = np.linalg.qr(np.random.randn(M, rank_spikes))\n",
        "\n",
        "# Les valeurs singulières du signal (choisies grandes pour être visibles)\n",
        "spike_vals = np.array([30, 40, 50, 60, 70])\n",
        "print(f\"Valeurs singulières injectées : {spike_vals}\")\n",
        "\n",
        "# Construction de S = U × Diag(spike_vals) × V^T\n",
        "S = U @ np.diag(spike_vals) @ V.T\n",
        "\n",
        "# Ajout du bruit : W = S + R\n",
        "W = S + R\n",
        "\n",
        "# Analyse spectrale de W\n",
        "eigs_W = compute_esd(W)\n",
        "sv_W = linalg.svdvals(W)  # Valeurs singulières (racine carrée des valeurs propres)\n",
        "\n",
        "print(f\"Seuil MP (en valeurs singulières) : {np.sqrt(lam_plus * N):.4f}\")\n",
        "print(f\"Nombre de spikes détectés au-dessus du seuil : {np.sum(sv_W > np.sqrt(lam_plus * N))}/{len(spike_vals)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBeGBi_8Mu83"
      },
      "outputs": [],
      "source": [
        "# Visualisation\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "# Histogramme des valeurs propres de W\n",
        "plt.hist(eigs_W, bins=60, density=True, alpha=0.6, label='ESD de W')\n",
        "\n",
        "# Théorie MP (qui prédit le bulk)\n",
        "plt.plot(x, marchenko_pastur(x, 1.0 / N, M / N)[0], 'r-', lw=2, label='Bulk MP')\n",
        "\n",
        "# Seuil λ+\n",
        "plt.axvline(lam_plus, color='g', ls='--', lw=2, label=f'λ+')\n",
        "\n",
        "plt.xlabel('λ (valeur propre)')\n",
        "plt.ylabel('Densité')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.title('W = S + R (spikes visibles à droite)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Les spikes apparaissent comme des pics isolés à droite du bulk !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81IxGCCyMu89"
      },
      "source": [
        "## 5. Test 3 : Réseau de Neurones sur MNIST\n",
        "\n",
        "### L'objectif\n",
        "\n",
        "Maintenant on passe aux choses sérieuses : on va entraîner un vrai réseau de neurones sur MNIST (reconnaissance de chiffres manuscrits) et analyser ses poids avec la loi MP.\n",
        "\n",
        "### Architecture du réseau\n",
        "\n",
        "Un MLP (Multi-Layer Perceptron) simple :\n",
        "- Entrée : 784 pixels (images 28×28)\n",
        "- Couche 1 : 784 → 500 neurones + ReLU\n",
        "- Couche 2 : 500 → 300 neurones + ReLU  \n",
        "- Sortie : 300 → 10 classes (chiffres 0-9)\n",
        "\n",
        "### Plan\n",
        "\n",
        "1. Charger les données MNIST\n",
        "2. Créer le réseau\n",
        "3. L'entraîner pendant 5 époques\n",
        "4. Analyser les poids de chaque couche"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH9z8HNBMu89"
      },
      "outputs": [],
      "source": [
        "print(\"TEST 3 : Analyse d'un réseau entraîné sur MNIST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Définition du modèle\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Réseau de neurones multi-couches pour MNIST.\n",
        "\n",
        "    Architecture :\n",
        "    - Linear 784 → 500\n",
        "    - ReLU\n",
        "    - Linear 500 → 300\n",
        "    - ReLU\n",
        "    - Linear 300 → 10\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(784, 500), nn.ReLU(),\n",
        "            nn.Linear(500, 300), nn.ReLU(),\n",
        "            nn.Linear(300, 10))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Aplatir l'image 28×28 en vecteur de 784 éléments\n",
        "        return self.net(x.view(x.size(0), -1))\n",
        "\n",
        "print(\"✓ Modèle défini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqZUgZL1Mu89"
      },
      "outputs": [],
      "source": [
        "# Chargement des données MNIST\n",
        "print(\"Chargement des données MNIST...\")\n",
        "\n",
        "# Transformations : conversion en tenseur + normalisation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Moyenne et écart-type de MNIST\n",
        "])\n",
        "\n",
        "# Téléchargement des données\n",
        "train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST('./data', train=False, download=False, transform=transform)\n",
        "\n",
        "# DataLoaders pour itérer sur les données\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n",
        "\n",
        "print(f\"✓ Données chargées : {len(train_data)} exemples d'entraînement, {len(test_data)} de test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjJLtyDlMu9D"
      },
      "outputs": [],
      "source": [
        "# Entraînement du réseau\n",
        "print(\"\\nDébut de l'entraînement...\\n\")\n",
        "\n",
        "model = MLP()\n",
        "criterion = nn.CrossEntropyLoss()  # Fonction de perte pour classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9)  # Optimiseur SGD\n",
        "\n",
        "for epoch in range(5):\n",
        "    # Phase d'entraînement\n",
        "    model.train()\n",
        "    for imgs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Réinitialiser les gradients\n",
        "        loss = criterion(model(imgs), labels)  # Calculer la perte\n",
        "        loss.backward()  # Rétropropagation\n",
        "        optimizer.step()  # Mise à jour des poids\n",
        "\n",
        "    # Phase d'évaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():  # Pas besoin de calculer les gradients ici\n",
        "        for imgs, labels in test_loader:\n",
        "            preds = model(imgs).argmax(1)  # Prédiction = classe avec score max\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / len(test_data)\n",
        "    print(f\"Epoch {epoch+1}/5 - Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "print(\"\\n✓ Entraînement terminé !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhXO6gVIMu9D"
      },
      "source": [
        "## 6. Analyse Spectrale des Poids du Réseau\n",
        "\n",
        "### Ce qu'on va faire\n",
        "\n",
        "Pour chaque couche du réseau, on va :\n",
        "1. Récupérer la matrice de poids W\n",
        "2. Calculer ses valeurs singulières\n",
        "3. Calculer le seuil MP pour cette couche\n",
        "4. Compter combien de valeurs singulières sont au-dessus du seuil\n",
        "\n",
        "### Interprétation\n",
        "\n",
        "Les valeurs singulières **au-dessus** du seuil MP représentent le signal utile que le réseau a appris. Celles **en-dessous** sont du bruit qu'on peut supprimer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZCB6NOuMu9D"
      },
      "outputs": [],
      "source": [
        "print(\"\\nAnalyse spectrale des couches :\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# On récupère toutes les matrices de poids (pas les biais)\n",
        "weight_params = [(n, p) for n, p in model.named_parameters() if 'weight' in n]\n",
        "\n",
        "for idx, (name, param) in enumerate(weight_params):\n",
        "    # Récupération de la matrice de poids\n",
        "    W = param.detach().cpu().numpy()\n",
        "    Nw, Mw = W.shape\n",
        "\n",
        "    # Calcul des valeurs singulières\n",
        "    sv = linalg.svdvals(W)\n",
        "\n",
        "    # Calcul des valeurs propres pour comparaison\n",
        "    eigs = compute_esd(W)\n",
        "\n",
        "    # Calcul du seuil MP\n",
        "    _, _, lam_plus = marchenko_pastur(None, 1.0 / Nw, Mw / Nw)\n",
        "    threshold = np.sqrt(lam_plus * Nw)  # Seuil en valeurs singulières\n",
        "\n",
        "    # Visualisation\n",
        "    ax = axes[idx]\n",
        "    ax.plot(np.arange(len(sv)), sv, 'o', ms=3, alpha=0.6, label='Valeurs singulières')\n",
        "    ax.axhline(threshold, color='r', ls='--', lw=2, label=f'Seuil MP={threshold:.3f}')\n",
        "    ax.set_yscale('log')  # Échelle logarithmique pour mieux voir\n",
        "    ax.set_xlabel('Indice')\n",
        "    ax.set_ylabel('Valeur singulière')\n",
        "    ax.set_title(f'{name} ({Nw}×{Mw})')\n",
        "    ax.legend()\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "    # Comptage des valeurs au-dessus du seuil\n",
        "    n_signal = int(np.sum(sv > threshold))\n",
        "    total = min(Nw, Mw)\n",
        "    percentage = 100 * n_signal / total\n",
        "\n",
        "    print(f\"{name:20s}: {n_signal:3d}/{total} val. sing. > seuil ({percentage:.1f}%)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Les points au-dessus de la ligne rouge sont le signal, ceux en-dessous sont du bruit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D2S0TZ4Mu9J"
      },
      "source": [
        "## 7. Pruning (Élagage) Basé sur la Loi MP\n",
        "\n",
        "### L'idée\n",
        "\n",
        "Maintenant qu'on sait quelles valeurs singulières sont du bruit, on va créer une version élaguée du réseau en :\n",
        "1. Faisant une SVD (décomposition en valeurs singulières) de chaque matrice de poids\n",
        "2. Mettant à zéro toutes les valeurs singulières en-dessous du seuil MP\n",
        "3. Reconstruisant la matrice avec seulement le signal\n",
        "\n",
        "### Avantages\n",
        "\n",
        "- Réseau plus petit (moins de paramètres effectifs)\n",
        "- Plus rapide à l'inférence\n",
        "- Peut généraliser mieux (moins d'overfitting)\n",
        "- Perte de performance minimale si bien fait"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSL_nye_Mu9J"
      },
      "outputs": [],
      "source": [
        "print(\"\\nPruning MP-based :\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Création d'une copie du modèle\n",
        "pruned_model = MLP()\n",
        "pruned_model.load_state_dict(model.state_dict())\n",
        "\n",
        "for name, param in pruned_model.named_parameters():\n",
        "    if 'weight' in name:\n",
        "        # Récupération de la matrice\n",
        "        W = param.detach().cpu().numpy()\n",
        "        Nw, Mw = W.shape\n",
        "\n",
        "        # Calcul du seuil MP\n",
        "        _, _, lam_plus = marchenko_pastur(None, 1.0 / Nw, Mw / Nw)\n",
        "        threshold = np.sqrt(lam_plus * Nw)\n",
        "\n",
        "        # SVD : W = U × S × V^T\n",
        "        U, s, Vt = linalg.svd(W, full_matrices=False)\n",
        "\n",
        "        # Mise à zéro des valeurs singulières < seuil\n",
        "        s_pruned = s.copy()\n",
        "        s_pruned[s_pruned < threshold] = 0.0\n",
        "\n",
        "        # Reconstruction de W\n",
        "        W_approx = (U * s_pruned) @ Vt  # Broadcasting efficace\n",
        "\n",
        "        # Mise à jour des poids du modèle\n",
        "        with torch.no_grad():\n",
        "            param.data.copy_(torch.from_numpy(W_approx).to(param.data.dtype))\n",
        "\n",
        "        n_kept = int(np.sum(s_pruned > 0))\n",
        "        total = min(Nw, Mw)\n",
        "        print(f\"{name:20s}: {n_kept:3d}/{total} val. sing. gardées\")\n",
        "\n",
        "print(\"\\n✓ Pruning terminé !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI3j7rlZMu9J"
      },
      "source": [
        "## 8. Évaluation : Comparaison des Performances\n",
        "\n",
        "### Question importante\n",
        "\n",
        "Est-ce que notre élagage a nui aux performances du réseau ?\n",
        "\n",
        "On va comparer l'accuracy (précision) du modèle original et du modèle élagué sur le jeu de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAT0pNkYMu9O"
      },
      "outputs": [],
      "source": [
        "# Fonction d'évaluation\n",
        "def evaluate(m):\n",
        "    \"\"\"\n",
        "    Calcule la précision d'un modèle sur le jeu de test MNIST.\n",
        "\n",
        "    Paramètres :\n",
        "    -----------\n",
        "    m : nn.Module\n",
        "        Le modèle à évaluer\n",
        "\n",
        "    Retour :\n",
        "    --------\n",
        "    accuracy : float\n",
        "        Précision en pourcentage (0-100)\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_loader:\n",
        "            preds = m(imgs).argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "    return 100 * correct / len(test_data)\n",
        "\n",
        "# Évaluation des deux modèles\n",
        "model.eval()\n",
        "pruned_model.eval()\n",
        "\n",
        "acc_orig = evaluate(model)\n",
        "acc_pruned = evaluate(pruned_model)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RÉSULTATS FINAUX\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy originale       : {acc_orig:.2f}%\")\n",
        "print(f\"Accuracy après pruning   : {acc_pruned:.2f}%\")\n",
        "print(f\"Différence               : {acc_pruned - acc_orig:+.2f}%\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if acc_pruned >= acc_orig - 1.0:\n",
        "    print(\"\\n✓ SUCCÈS : Le pruning MP préserve les performances !\")\n",
        "else:\n",
        "    print(\"\\n⚠ ATTENTION : Le pruning a causé une baisse de performance significative.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SID8Z_2WMu9P"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "### Ce qu'on a appris\n",
        "\n",
        "1. **La loi de Marchenko-Pastur** décrit comment se distribuent les valeurs propres d'une matrice aléatoire\n",
        "\n",
        "2. Dans un réseau de neurones entraîné, les poids contiennent :\n",
        "   - Du **signal** (valeurs singulières au-dessus du seuil MP)\n",
        "   - Du **bruit** (valeurs singulières en-dessous du seuil)\n",
        "\n",
        "3. On peut **élaguer** le réseau en supprimant le bruit sans perdre beaucoup de performance\n",
        "\n",
        "### Applications pratiques\n",
        "\n",
        "- Compression de modèles pour mobile/edge devices\n",
        "- Accélération de l'inférence\n",
        "- Meilleure généralisation (réduction de l'overfitting)\n",
        "- Compréhension de ce que le réseau a vraiment appris\n",
        "\n",
        "### Pour aller plus loin\n",
        "\n",
        "- Tester sur d'autres architectures (CNN, Transformers)\n",
        "- Comparer avec d'autres méthodes de pruning (magnitude, gradient-based)\n",
        "- Fine-tuning après pruning pour récupérer de la performance\n",
        "- Analyse de la stabilité de la solution"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}